{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee375d5",
   "metadata": {},
   "source": [
    "# Before going to the main content ...\n",
    "let's check if nvcc is available.<br>\n",
    "If it shows a report on the Cuda compiler driver, then we are good to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd7ddeb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\r\n",
      "Built on Thu_Nov_18_09:45:30_PST_2021\r\n",
      "Cuda compilation tools, release 11.5, V11.5.119\r\n",
      "Build cuda_11.5.r11.5/compiler.30672275_0\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59957f4f",
   "metadata": {},
   "source": [
    "# Hello world!\n",
    "\n",
    "Let's try to run the hello-world program.<br>\n",
    "Let me explain the code first.<br>\n",
    "The code is located in this directory : ```codes/1_introduction/```<br>\n",
    "It is a ```.cu``` file, which looks very similar to a ```.c``` file.<br>\n",
    "However, some necessary changes have to be made.\n",
    "- The function that runs in GPU is a ```__global__ void``` type.\n",
    "- GPU functions ALWAYS have to be ```void``` type. (please confirm this!)\n",
    "- The syntax for calling the GPU function is : ```func <<<num1, num2>>>();``` (I'll explain what these numbers are.)\n",
    "- The output of the GPU is collected after synchronizing with the CPU by doing ```cudaDeviceSynchronize();```.<br>\n",
    "\n",
    "The syntax for compiling and running is the following. <br>\n",
    "```nvcc -o output_exe input_code -run```<br>\n",
    "\n",
    "In this particular example, there are three parts.<br>\n",
    "\n",
    "#### A function that runs on the CPU:\n",
    "```\n",
    "void helloCPU(){\n",
    "    printf(\"Hello from the CPU.\\n\");\n",
    "}\n",
    "```\n",
    "#### A function that runs on the GPU:\n",
    "\n",
    "```\n",
    "__global__ void helloGPU(){\n",
    "    printf(\"Hello from the GPU.\\n\");\n",
    "}\n",
    "```\n",
    "#### A main function:\n",
    "```\n",
    "int main(){\n",
    "    helloCPU();\n",
    "    helloGPU<<<1, 1>>>();\n",
    "    cudaDeviceSynchronize();\n",
    "}\n",
    "```\n",
    "Inside the main function, after compilation, the CPU runs the ```helloCPU()``` function just like a regular C program. Then the ```helloGPU()``` function is sent to block=1 and thread=1 of the GPU. So this only runs once. (This is explained in the next block.) After that, ```cudaDeviceSynchronize()``` makes sure that the GPU and the CPU are synchronized. Now, the output from the GPU is available to the CPU, so that \"Hello from the GPU\" can be printed on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e57140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from the CPU.\n",
      "Hello from the GPU.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o executables/hello codes/1_introduction/hello.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b89e5",
   "metadata": {},
   "source": [
    "# CUDA Thread Hierarchy\n",
    "\n",
    "CUDA stands for Compute Unified Device Architecture. A **thread** refers to the smallest unit of work that can be scheduled and executed by a GPU. These are organized into various levels of hierarchy to efficiently utilize the GPU's parallel processing capabilities. Thousands or even millions of threads can run simultaneously on a GPU, allowing for massive parallelism.\n",
    "Threads within a block can cooperate and communicate with each other through shared memory.\n",
    "\n",
    "Threads are grouped into **blocks**. A block is a logical unit that provides synchronization, communication, and memory sharing among its threads. You can think of it as a collection of threads working together. All threads within a block can access the same shared memory. The maximum number of threads per block depends on the GPU architecture.\n",
    "\n",
    "A **grid** is a collection of blocks. Blocks within a grid can execute independently of each other, allowing for further parallelism. The blocks in a grid can be scheduled on any available multiprocessor (SM) within the GPU.\n",
    "\n",
    "Now, let's come back to the syntax : ```someKernel<<<num1, num2>>>();```\n",
    "Here, the GPU *kernel* is sent to the different threads from different blocks for parallel processing. The numbers here refers to the number of blocks and number of threads per block. i.e.,<br>\n",
    "```someKernel<<<NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>();```<br>\n",
    "\n",
    "**The kernel code is executed by every thread in every thread block configured when the kernel is launched**.<br>\n",
    "- `someKernel<<< 1,  1>>>()` is configured to run in a single thread block which has a single thread and will therefore run only once.\n",
    "- `someKernel<<< 1, 10>>>()` is configured to run in a single thread block which has 10 threads and will therefore run 10 times.\n",
    "- `someKernel<<<10,  1>>>()` is configured to run in 10 thread blocks which each have a single thread and will therefore run 10 times.\n",
    "- `someKernel<<<10, 10>>>()` is configured to run in 10 thread blocks which each have 10 threads and will therefore run 100 times.\n",
    "\n",
    "Let's see this in action.\n",
    "\n",
    "### Playing with number of threads and blocks:\n",
    "\n",
    "The threads in each block and the blocks themselves are given indices (integers) which start from 0. These indices can be accessed through variables such as ```threadIdx.x``` and ```blockIdx.x```. The following is an example where I am printing out these indices for each thread. You can see that the print statements are 'not chronological'. They are being processed simultaneously in the threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e78399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread index (in each block) = 0, block index = 1 \r\n",
      "thread index (in each block) = 1, block index = 1 \r\n",
      "thread index (in each block) = 0, block index = 0 \r\n",
      "thread index (in each block) = 1, block index = 0 \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o executables/printing_numbers codes/1_introduction/threads_and_blocks.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268af8bf",
   "metadata": {},
   "source": [
    "### Making the code idiot-proof\n",
    "There is an issue with manually setting nBlocks and nThreads. The number of cores needed to execute the code is at least nBlocks * nThreadsPerBlock. That's why we can't just come up with some arbitrary number of blocks and thread per block. In my case, I have 896 cores in my GPU. So, the product, nBlocks * nThreadsPerBlock can't go beyond that.\n",
    "\n",
    "CUDA Kernels have access to a special variable that gives the number of threads in a block: `blockDim.x`. Using this variable, in conjunction with `blockIdx.x` and `threadIdx.x`, we can find out a unique ID for each thread in all the blocks by calculating this : `threadIdx.x + blockIdx.x * blockDim.x`. The following is a detailed example demonstrating how it works.\n",
    "\n",
    "Consider the execution configuration <<<10, 10>>> . It would launch a grid with a total of 100 threads, contained in 10 blocks of 10 threads. We would therefore hope for each thread to have the ability to calculate some index unique to itself between 0 and 99.\n",
    "\n",
    "- If `blockIdx.x == 0`, then `blockIdx.x * blockDim.x` is 0. Adding to 0 the possible threadIdx.x values 0 through 9, then we can generate the indices 0 through 9 within the 100 thread grid.\n",
    "- If `blockIdx.x == 1`, then `blockIdx.x * blockDim.x` is 10. Adding to 10 the possible threadIdx.x values 0 through 9, then we can generate the indices 10 through 19 within the 100 thread grid.\n",
    "- If `blockIdx.x == 5`, then `blockIdx.x * blockDim.x` is 50. Adding to 50 the possible threadIdx.x values 0 through 9, then we can generate the indices 50 through 59 within the 100 thread grid.\n",
    "- If `blockIdx.x == 9`, then `blockIdx.x * blockDim.x` is 90. Adding to 90 the possible threadIdx.x values 0 through 9, then we can generate the indices 90 through 99 within the 100 thread grid.\n",
    "\n",
    "This is how we end up with unique IDs for each thread which runs from 0 to 99. Once we have that, we can control how many parallel processing happens in the GPU.\n",
    "\n",
    "For example, first, we set how many cores to use.\n",
    "\n",
    "`int N = 896;`<br>\n",
    "\n",
    "This is the maxmium cores that I can use. I may choose a smaller number as well, depending on what I want to do. Assume that we have a desire to set threads_per_block exactly to 256. Then,<br>\n",
    "\n",
    "`size_t threads_per_block = 256;`<br>\n",
    "\n",
    "For a given number of cores to use, the number of blocks should be the following.<br> \n",
    "\n",
    "`size_t number_of_blocks = (N + threads_per_block - 1) / threads_per_block;`\n",
    "\n",
    "Inside the GPU function, we calculate the unique ID of the thread by doing the following.\n",
    "\n",
    "`int idx = threadIdx.x + blockIdx.x * blockDim.x;`\n",
    "\n",
    "Then, we tell the GPU to execute the job, only when `idx < N`.\n",
    "\n",
    "Let's modify the previous example to include this unique ID. I am printing out the integers from 0 to 9, and hence choosing 10 threads for this operation. Also, I am choosing the number of threads per block to be 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "119fe3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread index (in each block) = 0 , block index = 2, thread ID = 8 \r\n",
      "thread index (in each block) = 1 , block index = 2, thread ID = 9 \r\n",
      "thread index (in each block) = 0 , block index = 0, thread ID = 0 \r\n",
      "thread index (in each block) = 1 , block index = 0, thread ID = 1 \r\n",
      "thread index (in each block) = 2 , block index = 0, thread ID = 2 \r\n",
      "thread index (in each block) = 3 , block index = 0, thread ID = 3 \r\n",
      "thread index (in each block) = 0 , block index = 1, thread ID = 4 \r\n",
      "thread index (in each block) = 1 , block index = 1, thread ID = 5 \r\n",
      "thread index (in each block) = 2 , block index = 1, thread ID = 6 \r\n",
      "thread index (in each block) = 3 , block index = 1, thread ID = 7 \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o executables/printing_numbers_2 codes/1_introduction/threads_and_blocks_2.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e0c2e",
   "metadata": {},
   "source": [
    "Notice that, the thread IDs are not organised from 0 to 9. This is because the blocks are not organised that way. You can see that the threads in each block are organised from 0 to 3 (except for the last block, where the thread id goes from 0 to 1, as the number of cores used here is exhausted). \n",
    "\n",
    "You can do a fun exercise of printing out the integers from 0 to 9. But instead of using a for loop in CPU, where the print statment is executed one after another, I am using the index features of the GPU to execute the print statement simulataneously.\n",
    "\n",
    "I have two algorithms for doing that. The first one involves using 10 cores and printing out the unique thread id (which goes from 0 to 9). This is similar to the previous example. That's why I am not repreating it here. The second one involves a matrix of numbers {block index, thread index}, where only the numbers from the diagonal elements are being printed on screen. The GPU function will look like this.\n",
    "\n",
    "```\n",
    "__global__ void print_integers(){\n",
    "    int block_index = blockIdx.x;\n",
    "    int thread_index = threadIdx.x;\n",
    "    if(block_index == thread_index){ //for only the diagonal elements in the matrix {block index, thread index}\n",
    "        printf(\"%d\", block_index);\n",
    "    } \n",
    "}\n",
    "```\n",
    "Since we are printing out numbers from 0 to 9, both `block_index` and `thread_index` should have values from 0 to 9. That's why in the main function, we run the kernel as follows.\n",
    "`print_integers<<<10, 10>>>();`\n",
    "\n",
    "The first algorithm is better, because it prints out a number in each thread. In the second algorithm, it only prints out a number in some of the threads (where the block index matches the thread index). For a limited number of GPU cores, the first algorithm can print more numbers than the second one.\n",
    "\n",
    "Try this example yourself. The disadvantage of this method is that, it is limited by the number of available cores. That's why its important to make the code 'idiot proof' by executing the processes only when the thread id is less than the number of avaliable cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1507b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
